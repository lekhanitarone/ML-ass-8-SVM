{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is a Support Vector Machine (SVM), and how does it work?**\n",
        "\n",
        ">A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the best hyperplane that separates data points of different classes with the maximum margin. The data points closest to the hyperplane are called support vectors, and they influence the position and orientation of the boundary. SVM can handle both linear and non-linear data using kernel functions. It is effective in high-dimensional spaces and provides good generalization performance. SVM minimizes classification errors while maximizing the separation between classes. It is widely used in text classification, image recognition, and bioinformatics.\n",
        "\n",
        "**2. Explain the difference between Hard Margin and Soft Margin SVM.**\n",
        "\n",
        ">In a Hard Margin SVM, the goal is to find a hyperplane that separates all data points perfectly with no misclassifications. It assumes that the data is completely linearly separable and noise-free. However, it is very sensitive to outliers because even a single misclassified point can affect the boundary.\n",
        "\n",
        ">In a Soft Margin SVM, the model allows some errors or margin violations to improve flexibility and generalization. It introduces a penalty parameter (C) that balances the trade-off between achieving a larger margin and minimizing classification errors. This makes Soft Margin SVM suitable for real-world, noisy, or overlapping data. Overall, Soft Margin SVM provides better performance in practical applications.\n",
        "\n",
        " **3. What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.**\n",
        "\n",
        ">The Kernel Trick in SVM is a mathematical technique used to transform non-linearly separable data into a higher-dimensional space where it becomes linearly separable. Instead of explicitly computing the transformation, the kernel function calculates the inner product in the new feature space, saving computation time.\n",
        "\n",
        ">One common example is the Radial Basis Function (RBF) Kernel, which measures the similarity between data points based on their distance. It is especially useful for problems where the decision boundary is circular or complex, such as image classification or pattern recognition. The Kernel Trick allows SVM to handle complex, non-linear relationships efficiently.\n",
        "\n",
        " **4.What is a Naïve Bayes Classifier, and why is it called “naïve”?**\n",
        "\n",
        ">A Naïve Bayes Classifier is a probabilistic machine learning algorithm based on Bayes’ Theorem, mainly used for classification tasks. It works by calculating the probability of each class given a set of input features and then assigning the class with the highest probability to the data point. The classifier assumes that all features are independent of each other given the class label, which simplifies the computation and makes the algorithm fast and efficient. It is called “naïve” because this independence assumption is often unrealistic in real-world scenarios, where features can be correlated. However, despite this simplification, Naïve Bayes often performs surprisingly well, especially in applications like spam detection, sentiment analysis, document categorization, and medical diagnosis, due to its simplicity, scalability, and good performance on high-dimensional data.\n",
        "\n",
        "**5.Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?**\n",
        "\n",
        ">There are three main variants of the Naïve Bayes Classifier: Gaussian, Multinomial, and Bernoulli. The Gaussian Naïve Bayes is used when the features are continuous and follow a normal (Gaussian) distribution, making it suitable for datasets like medical measurements or sensor readings. The Multinomial Naïve Bayes is used for discrete features such as word counts or frequencies, commonly applied in text classification, document categorization, and spam filtering. The Bernoulli Naïve Bayes works with binary or boolean features, where data is represented as 0s and 1s to indicate the presence or absence of a feature. It is often used for sentiment analysis or document classification when only the occurrence of a word matters, not its frequency.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ts0ORgo4nnNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Dataset Info:\n",
        "# ● You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "# sklearn.datasets or a CSV file you have.\n",
        "\n",
        "# Question 6: Write a Python program to:\n",
        "# ● Load the Iris dataset\n",
        "# ● Train an SVM Classifier with a linear kernel\n",
        "# ● Print the model's accuracy and support vectors.\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the SVM model with a linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Support Vectors:\\n\", svm_model.support_vectors_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1QGDgtIpRsy",
        "outputId": "7020aa4d-97f5-4bbc-f843-685a11907426"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question (7)\n",
        "# Write a Python program to:\n",
        "# ● Load the Breast Cancer dataset\n",
        "# ● Train a Gaussian Naïve Bayes model\n",
        "# ● Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB9zKEd7rQMm",
        "outputId": "3c3ed704-0010-4f94-bdea-5e18add5e1bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.90      0.92        63\n",
            "           1       0.95      0.96      0.95       108\n",
            "\n",
            "    accuracy                           0.94       171\n",
            "   macro avg       0.94      0.93      0.94       171\n",
            "weighted avg       0.94      0.94      0.94       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question (8)\n",
        "# Write a Python program to:\n",
        "# ● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "# C and gamma.\n",
        "# ● Print the best hyperparameters and accuracy.\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for C and gamma\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Create the SVM model and apply GridSearchCV\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best hyperparameters and accuracy\n",
        "print(\"Best Hyperparameters:\", grid.best_params_)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Liddm-sdrc08",
        "outputId": "fdce9223-ffac-44e4-e3be-5379ae720b39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best Hyperparameters: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Model Accuracy: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question (9)\n",
        "# Write a Python program to:\n",
        "# ● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "# sklearn.datasets.fetch_20newsgroups).\n",
        "# ● Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the synthetic text dataset\n",
        "categories = ['sci.space', 'rec.autos', 'comp.graphics', 'talk.politics.mideast']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Naïve Bayes Classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict probabilities on test data\n",
        "y_proba = nb_model.predict_proba(X_test_tfidf)\n",
        "\n",
        "# Convert true labels to binary format for ROC-AUC computation\n",
        "y_test_bin = label_binarize(y_test, classes=range(len(categories)))\n",
        "\n",
        "# Compute ROC-AUC score (macro-average for multi-class)\n",
        "roc_auc = roc_auc_score(y_test_bin, y_proba, average='macro')\n",
        "\n",
        "# Display the ROC-AUC Score\n",
        "print(\"Naïve Bayes Classifier ROC-AUC Score:\", roc_auc)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qlKxhrpOr2Pb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        ">● Text with diverse vocabulary\n",
        "\n",
        ">● Potential class imbalance (far more legitimate emails than spam)\n",
        "\n",
        ">● Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "\n",
        ">● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "\n",
        ">● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "\n",
        ">● Address class imbalance\n",
        "\n",
        ">● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "**Answers :**\n",
        "\n",
        "Approach Explanation\n",
        "1. Preprocessing the Data\n",
        "\n",
        "Handling missing data: Replace missing email text with an empty string (\"\") or remove those rows.\n",
        "\n",
        "Text vectorization: Use TF-IDF Vectorization to convert text into numerical form while giving more importance to distinctive words.\n",
        "\n",
        "Lowercasing and stopword removal help clean the text.\n",
        "\n",
        "| Model                            | Pros                                                                   | Cons                                                         | Suitable for                        |\n",
        "| -------------------------------- | ---------------------------------------------------------------------- | ------------------------------------------------------------ | ----------------------------------- |\n",
        "| **Naïve Bayes (MultinomialNB)**  | Fast, efficient on text data, works well when features are independent | Assumes independence, can underperform if correlations exist | Large text datasets, quick baseline |\n",
        "| **SVM (Support Vector Machine)** | Handles high-dimensional data, robust to outliers                      | Slower on large data, needs careful tuning                   | Smaller or balanced datasets        |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. Handling Class Imbalance\n",
        "\n",
        "Use SMOTE (Synthetic Minority Oversampling Technique) or class weights.\n",
        "\n",
        "Alternatively, adjust thresholds or use stratified sampling.\n",
        "\n",
        "Here, we’ll use SMOTE to balance classes.\n",
        ">\n",
        "4. Evaluation Metrics\n",
        "\n",
        "Because of imbalance, accuracy alone isn’t enough.\n",
        "We use:\n",
        "\n",
        "Precision & Recall\n",
        "\n",
        "F1-score\n",
        "\n",
        "ROC-AUC score\n",
        "\n",
        "5. Business Impact\n",
        "\n",
        "Reduces manual filtering effort.\n",
        "\n",
        "Improves productivity by reducing spam exposure.\n",
        "\n",
        "Prevents phishing and malicious email clicks.\n",
        "\n",
        "Builds trust in internal email systems.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IxssrshYt-4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#python implementation\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "\n",
        "# ---- Step 1: Create a sample dataset (simulate email data) ----\n",
        "data = {\n",
        "    'email_text': [\n",
        "        \"Win a free iPhone now!\", \"Meeting at 10 AM\", \"Limited offer just for you\",\n",
        "        \"Project update attached\", \"Earn money from home\", \"Lunch tomorrow?\",\n",
        "        \"Congratulations, you've been selected!\", \"Please review the attached invoice\",\n",
        "        \"Exclusive deal, buy now!\", \"Schedule team call\"\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1=Spam, 0=Not Spam\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# ---- Step 2: Handle missing data ----\n",
        "df['email_text'] = df['email_text'].fillna('')\n",
        "\n",
        "# ---- Step 3: Split the data ----\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['email_text'], df['label'], test_size=0.3, random_state=42)\n",
        "\n",
        "# ---- Step 4: Vectorize text ----\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# ---- Step 5: Handle class imbalance using SMOTE ----\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_bal, y_train_bal = smote.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "# ---- Step 6: Train Naïve Bayes Model ----\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "# ---- Step 7: Predict and Evaluate ----\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "y_proba = model.predict_proba(X_test_tfidf)[:, 1]\n",
        "\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_proba))\n",
        "\n",
        "# Output :\n",
        "\n",
        "# Classification Report:\n",
        "#               precision    recall  f1-score   support\n",
        "\n",
        "#            0       1.00      1.00      1.00         2\n",
        "#            1       1.00      1.00      1.00         1\n",
        "\n",
        "#     accuracy                           1.00         3\n",
        "#    macro avg       1.00      1.00      1.00         3\n",
        "# weighted avg       1.00      1.00      1.00         3\n",
        "\n",
        "# ROC-AUC Score: 1.0\n"
      ],
      "metadata": {
        "id": "wL3k4k2vv1Ml"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}